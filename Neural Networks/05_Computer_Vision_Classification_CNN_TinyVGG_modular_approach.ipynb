{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPTVftsc7xPtt2paqU1gmQS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Computer Vision Classification CNN TinyVGG model on Food101 extract dataset - modular approach\n","\n","This notebook enables to generate python scripts which are then used to train a model."],"metadata":{"id":"1WZ8Mdmn46dE"}},{"cell_type":"markdown","source":["## 1. Get data"],"metadata":{"id":"dWUPjGz25NBb"}},{"cell_type":"code","source":["# Create a directory for modular_approach scripts\n","\n","import os\n","os.makedirs(\"modular_approach\", exist_ok=True)"],"metadata":{"id":"MXoWRAwlClEN","executionInfo":{"status":"ok","timestamp":1700770801959,"user_tz":-60,"elapsed":258,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["%%writefile modular_approach/data_download.py\n","\"\"\"\n","Contains functionality to download and unzip prepared data\n","for training and testing from target URL.\n","\"\"\"\n","\n","import requests\n","import zipfile\n","from pathlib import Path\n","\n","# Setup path to a data folder\n","data_path = Path(\"data/\")\n","image_path = data_path / \"food101_extract\"\n","\n","# Create data folder if doesn't exist\n","if image_path.is_dir():\n","  print(f\"{image_path} directory already exists\")\n","else:\n","  image_path.mkdir(parents=True, exist_ok=True)\n","  print(f\"{image_path} directory created\")\n","\n","# Download food101_extract data\n","with open(data_path / \"food101_extract.zip\", \"wb\") as f:\n","  request = requests.get(\"https://github.com/slawomirwojtas/ML-Projects/raw/main/food101_extract.zip\")\n","  print(\"Downloading food101_extract data...\")\n","  f.write(request.content)\n","\n","# Unzip food101_extract data\n","with zipfile.ZipFile(data_path / \"food101_extract.zip\", \"r\") as zip_ref:\n","  print(\"Unzipping food101_extract data...\")\n","  zip_ref.extractall(image_path)\n","  print(\"Done\")"],"metadata":{"id":"vsJhayXp5NDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700770816548,"user_tz":-60,"elapsed":251,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"a4d9b793-69ff-40d1-f97f-6451a9a6b3c7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing modular_approach/data_download.py\n"]}]},{"cell_type":"markdown","source":["## 2. Create Datasets and DataLoaders"],"metadata":{"id":"ZgYaVDaQ5NGH"}},{"cell_type":"code","source":["%%writefile modular_approach/data_setup.py\n","\"\"\"\n","Contains functionality for creating PyTorch DataLoaders for image classification data.\n","\"\"\"\n","\n","import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","NUM_WORKERS = os.cpu_count()\n","\n","\n","\n","def create_dataloaders(\n","  train_dir: str,\n","  test_dir: str,\n","  transform: transforms.Compose,\n","  batch_size: int,\n","  num_workers: int=NUM_WORKERS\n","  ):\n","  \"\"\"\n","  Creates training and testing DataLoaders.\n","\n","  Takes in a training directory and testing directpry path and turns them into\n","  PyTorch Datasets and then into PyTorch DataLoaders.\n","\n","  Args:\n","    train_dir: Path to training directory.\n","    test_dir: Path to testing directory.\n","    transform: torchvision transforms to perform on training and testing data.\n","    batch_size: Number of samples per batch in each of the DataLoaders.\n","    num_workers: An integer for number of workers per DataLoader.\n","\n","  Returns:\n","    A tuple of (train_dataloader, test_dataloader, class_names).\n","    Where class_names is a list of the target classes.\n","    Example usage:\n","      train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=path/to/train_dir,\n","        test_dir=path/to/test_dir,\n","        transform=some_transform,\n","        batch_size=32,\n","        num_workers=4)\n","  \"\"\"\n","\n","  # Use ImageFolder to create datasets\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into DataLoaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True\n","  )\n","\n","  test_dataloader = DataLoader(\n","    test_data,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=num_workers,\n","    pin_memory=True\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names\n","\n","\n"],"metadata":{"id":"cNWv18XS50JG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700770829129,"user_tz":-60,"elapsed":287,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"58615b67-ccf3-4762-97dd-3990230ef51e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing modular_approach/data_setup.py\n"]}]},{"cell_type":"markdown","source":["## 3. Making a model (TinyVGG for 224x224 px input)"],"metadata":{"id":"Vqs4bcAG50LT"}},{"cell_type":"code","source":["%%writefile modular_approach/model_builder.py\n","\"\"\"\n","Contains PyTorh model code to instantiate a TinyVGG model from the CNN Explainer website.\n","\"\"\"\n","\n","import torch\n","from torch import nn\n","\n","class TinyVGG(nn.Module):\n","  \"\"\"Creates the TinyVGG architecture.\n","\n","  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n","  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n","\n","  Args:\n","    input_shape: An integer indicating number of input channels.\n","    hidden_units: An integer indicating number of hidden units between layers.\n","    output_shape: An integer indicating number of output units.\n","  \"\"\"\n","  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","      super().__init__()\n","      self.conv_block_1 = nn.Sequential(\n","          nn.Conv2d(in_channels=input_shape,\n","                    out_channels=hidden_units,\n","                    kernel_size=3, # how big is the square that's going over the image?\n","                    stride=1, # default\n","                    padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n","          nn.ReLU(),\n","          nn.Conv2d(in_channels=hidden_units,\n","                    out_channels=hidden_units,\n","                    kernel_size=3,\n","                    stride=1,\n","                    padding=0),\n","          nn.ReLU(),\n","          nn.MaxPool2d(kernel_size=2,\n","                        stride=2) # default stride value is same as kernel_size\n","      )\n","      self.conv_block_2 = nn.Sequential(\n","          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n","          nn.ReLU(),\n","          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n","          nn.ReLU(),\n","          nn.MaxPool2d(2)\n","      )\n","      self.classifier = nn.Sequential(\n","          nn.Flatten(),\n","          # Where did this in_features shape come from?\n","          # It's because each layer of our network compresses and changes the shape of our inputs data.\n","          nn.Linear(in_features=hidden_units*53*53,\n","                    out_features=output_shape)\n","      )\n","\n","  def forward(self, x: torch.Tensor):\n","      x = self.conv_block_1(x)\n","      x = self.conv_block_2(x)\n","      x = self.classifier(x)\n","      return x\n","      # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion\n"],"metadata":{"id":"AW6R9OKI50Nt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700770832357,"user_tz":-60,"elapsed":268,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"7d46e989-703e-499f-9873-0673149a316d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing modular_approach/model_builder.py\n"]}]},{"cell_type":"markdown","source":["## 4. Creating engine for training and testing steps combined with single train function"],"metadata":{"id":"mwuf0dHf50QR"}},{"cell_type":"code","source":["%%writefile modular_approach/engine.py\n","\"\"\"\n","Contains functions for training and testing a PyTorch model.\n","\"\"\"\n","from typing import Dict, List, Tuple\n","import torch\n","from tqdm.auto import tqdm\n","\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device) -> Tuple[float, float]:\n","  \"\"\"Trains a PyTorch model for a single epoch.\n","\n","  Turns a target PyTorch model to training mode and then\n","  runs through all of the required training steps (forward\n","  pass, loss calculation, optimizer step).\n","\n","  Args:\n","    model: A PyTorch model to be trained.\n","    dataloader: A DataLoader instance for the model to be trained on.\n","    loss_fn: A PyTorch loss function to minimize.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A tuple of training loss and training accuracy metrics.\n","    In the form (train_loss, train_accuracy). For example:\n","\n","    (0.1112, 0.8743)\n","  \"\"\"\n","  # Put model in train mode\n","  model.train()\n","\n","  # Setup train loss and train accuracy values\n","  train_loss, train_acc = 0, 0\n","\n","  # Loop through data loader data batches\n","  for batch, (X, y) in enumerate(dataloader):\n","      # Send data to target device\n","      X, y = X.to(device), y.to(device)\n","\n","      # 1. Forward pass\n","      y_pred = model(X)\n","\n","      # 2. Calculate  and accumulate loss\n","      loss = loss_fn(y_pred, y)\n","      train_loss += loss.item()\n","\n","      # 3. Optimizer zero grad\n","      optimizer.zero_grad()\n","\n","      # 4. Loss backward\n","      loss.backward()\n","\n","      # 5. Optimizer step\n","      optimizer.step()\n","\n","      # Calculate and accumulate accuracy metric across all batches\n","      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  train_loss = train_loss / len(dataloader)\n","  train_acc = train_acc / len(dataloader)\n","  return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              device: torch.device) -> Tuple[float, float]:\n","  \"\"\"Tests a PyTorch model for a single epoch.\n","\n","  Turns a target PyTorch model to \"eval\" mode and then performs\n","  a forward pass on a testing dataset.\n","\n","  Args:\n","    model: A PyTorch model to be tested.\n","    dataloader: A DataLoader instance for the model to be tested on.\n","    loss_fn: A PyTorch loss function to calculate loss on the test data.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A tuple of testing loss and testing accuracy metrics.\n","    In the form (test_loss, test_accuracy). For example:\n","\n","    (0.0223, 0.8985)\n","  \"\"\"\n","  # Put model in eval mode\n","  model.eval()\n","\n","  # Setup test loss and test accuracy values\n","  test_loss, test_acc = 0, 0\n","\n","  # Turn on inference context manager\n","  with torch.inference_mode():\n","      # Loop through DataLoader batches\n","      for batch, (X, y) in enumerate(dataloader):\n","          # Send data to target device\n","          X, y = X.to(device), y.to(device)\n","\n","          # 1. Forward pass\n","          test_pred_logits = model(X)\n","\n","          # 2. Calculate and accumulate loss\n","          loss = loss_fn(test_pred_logits, y)\n","          test_loss += loss.item()\n","\n","          # Calculate and accumulate accuracy\n","          test_pred_labels = test_pred_logits.argmax(dim=1)\n","          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  test_loss = test_loss / len(dataloader)\n","  test_acc = test_acc / len(dataloader)\n","  return test_loss, test_acc\n","\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device) -> Dict[str, List[float]]:\n","  \"\"\"Trains and tests a PyTorch model.\n","\n","  Passes a target PyTorch models through train_step() and test_step()\n","  functions for a number of epochs, training and testing the model\n","  in the same epoch loop.\n","\n","  Calculates, prints and stores evaluation metrics throughout.\n","\n","  Args:\n","    model: A PyTorch model to be trained and tested.\n","    train_dataloader: A DataLoader instance for the model to be trained on.\n","    test_dataloader: A DataLoader instance for the model to be tested on.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","    epochs: An integer indicating how many epochs to train for.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A dictionary of training and testing loss as well as training and\n","    testing accuracy metrics. Each metric has a value in a list for\n","    each epoch.\n","    In the form: {train_loss: [...],\n","                  train_acc: [...],\n","                  test_loss: [...],\n","                  test_acc: [...]}\n","    For example if training for epochs=2:\n","                 {train_loss: [2.0616, 1.0537],\n","                  train_acc: [0.3945, 0.3945],\n","                  test_loss: [1.2641, 1.5706],\n","                  test_acc: [0.3400, 0.2973]}\n","  \"\"\"\n","  # Create empty results dictionary\n","  results = {\"train_loss\": [],\n","      \"train_acc\": [],\n","      \"test_loss\": [],\n","      \"test_acc\": []\n","  }\n","\n","  # Loop through training and testing steps for a number of epochs\n","  for epoch in tqdm(range(epochs)):\n","      train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","      test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","\n","      # Print out what's happening\n","      print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","      )\n","\n","      # Update results dictionary\n","      results[\"train_loss\"].append(train_loss)\n","      results[\"train_acc\"].append(train_acc)\n","      results[\"test_loss\"].append(test_loss)\n","      results[\"test_acc\"].append(test_acc)\n","\n","  # Return the filled results at the end of the epochs\n","  return results\n"],"metadata":{"id":"wdfFxFpO50Su","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700770852521,"user_tz":-60,"elapsed":240,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"c2d45acb-e27a-4e1d-a430-dd552083ec23"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting modular_approach/engine.py\n"]}]},{"cell_type":"markdown","source":["## 5. Utility functions"],"metadata":{"id":"D64mzsTm50VF"}},{"cell_type":"code","source":["%%writefile modular_approach/utils.py\n","\"\"\"\n","File containing various utility functions for PyTorch model training.\n","\"\"\"\n","\n","import torch\n","from pathlib import Path\n","\n","\n","def save_model(model: torch.nn.Module,\n","               target_dir: str,\n","               model_name: str):\n","  \"\"\"Saves a PyTorch model to a target directory.\n","\n","  Args:\n","    model: A target PyTorch model to save.\n","    target_dir: A directory for saving the model to.\n","    model_name: A filename for the saved model. Should include\n","      either \".pth\" or \".pt\" as the file extension.\n","\n","  Example usage:\n","    save_model(model=model_0,\n","               target_dir=\"models\",\n","               model_name=\"05_going_modular_tinygvgg_model.pth\")\n","  \"\"\"\n","  # Create target directory\n","  target_dir_path = Path(target_dir)\n","  target_dir_path.mkdir(parents=True,\n","                        exist_ok=True)\n","\n","  # Create model save path\n","  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n","  model_save_path = target_dir_path / model_name\n","\n","  # Save the model state_dict()\n","  print(f\"[INFO] Saving model to: {model_save_path}\")\n","  torch.save(obj=model.state_dict(),\n","             f=model_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmtGV_maJ3xZ","executionInfo":{"status":"ok","timestamp":1700770862872,"user_tz":-60,"elapsed":253,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"a3ae909c-7c2e-4587-ea93-558b157bb86e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing modular_approach/utils.py\n"]}]},{"cell_type":"markdown","source":["## 6. Train, evaluate and save the model -> `main.py`"],"metadata":{"id":"d6DLXDs-J3ze"}},{"cell_type":"code","source":["%%writefile modular_approach/main.py\n","\"\"\"\n","Trains a PyTorch image classification model using device-agnostic code.\n","\"\"\"\n","\n","import os\n","import argparse\n","import torch\n","from torchvision import transforms\n","import data_setup, engine, model_builder, utils\n","from timeit import default_timer as timer\n","\n","# Create a parser\n","parser = argparse.ArgumentParser(description=\"Get hyperparameters\")\n","\n","# Get an arg for NUM_EPOCHS\n","parser.add_argument(\"--num_epochs\",\n","                    default=5,\n","                    type=int,\n","                    help=\"number of epochs to train for\")\n","\n","# Get an arg for BATCH_SIZE\n","parser.add_argument(\"--batch_size\",\n","                    default=32,\n","                    type=int,\n","                    help=\"number of samples per batch\")\n","\n","# Get an arg for HIDDEN_UNITS\n","parser.add_argument(\"--hidden_units\",\n","                    default=10,\n","                    type=int,\n","                    help=\"number of neurons in hidden layers\")\n","\n","# Get an arg for LEARNING_RATE\n","parser.add_argument(\"--learning_rate\",\n","                    default=0.001,\n","                    type=float,\n","                    help=\"learning rate for the optimizer\")\n","\n","# Get an arg for train_dir\n","parser.add_argument(\"--train_dir\",\n","                    default=\"data/food101_extract/train\",\n","                    type=str,\n","                    help=\"directory file path to training data in standard image classification format\")\n","\n","# Get an arg for test_dir\n","parser.add_argument(\"--test_dir\",\n","                    default=\"data/food101_extract/test\",\n","                    type=str,\n","                    help=\"directory file path to testing data in standard image classification format\")\n","\n","# Get the arguments from the parser\n","args = parser.parse_args()\n","\n","# Setup hyperparameters\n","NUM_EPOCHS = args.num_epochs\n","BATCH_SIZE = args.batch_size\n","HIDDEN_UNITS = args.hidden_units\n","LEARNING_RATE = args.learning_rate\n","print(f\"[INFO] Training a model for {NUM_EPOCHS} epochs with batch size {BATCH_SIZE} using {HIDDEN_UNITS} hidden units and a learning rate of {LEARNING_RATE}\")\n","\n","\n","# Setup directories\n","train_dir = args.train_dir\n","test_dir = args.test_dir\n","print(f\"[INFO] Training data file: {train_dir}\")\n","print(f\"[INFO] Testing data file: {test_dir}\")\n","\n","# Setup device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Create transforms\n","data_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.ToTensor()\n","])\n","\n","\n","# Create DataLoaders and get class_names\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n","                                                                               test_dir=test_dir,\n","                                                                               transform=data_transform,\n","                                                                               batch_size=BATCH_SIZE)\n","\n","# Create a model\n","model = model_builder.TinyVGG(input_shape=3,\n","                              hidden_units=HIDDEN_UNITS,\n","                              output_shape=len(class_names)).to(device)\n","\n","# Setup loss and optimizer\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                             lr=LEARNING_RATE)\n","\n","# Start the timer\n","start_time = timer()\n","\n","\n","# Start training with help from engine.py\n","model_results = engine.train(model=model,\n","                             train_dataloader=train_dataloader,\n","                             test_dataloader=test_dataloader,\n","                             optimizer=optimizer,\n","                             loss_fn=loss_fn,\n","                             epochs=NUM_EPOCHS,\n","                             device=device)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n","\n","\n","# Save the model to file\n","utils.save_model(model=model,\n","                 target_dir=\"models\",\n","                 model_name=\"05_modular_approach_tinyvgg_model.pth\")\n","print(\"model saved\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEKxJMhgJ310","executionInfo":{"status":"ok","timestamp":1700770873567,"user_tz":-60,"elapsed":248,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"6a0069be-0cc6-4390-cab8-0f339e8c40bd"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing modular_approach/main.py\n"]}]},{"cell_type":"markdown","source":["## 7. Predict result from target URL"],"metadata":{"id":"XGWjPK3Y5NIq"}},{"cell_type":"code","source":["%%writefile modular_approach/predict.py\n","\n","\"\"\"\n","Predicts class for a target image URL from a trained model\n","\"\"\"\n","\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import requests\n","import argparse\n","import model_builder\n","\n","# Setup device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Creating a parser\n","parser = argparse.ArgumentParser()\n","\n","# Get an IMAGE_URL\n","parser.add_argument(\"--image_url\",\n","                    default=\"https://na-talerzu.pl/wp-content/uploads/2022/08/Paella-z-krewetkami-i-chorizo-0849-2.jpg\",\n","                    type=str,\n","                    help=\"target image url to predict on\")\n","\n","# Get a model path\n","parser.add_argument(\"--model_path\",\n","                    default=\"models/05_modular_approach_tinyvgg_model.pth\",\n","                    type=str,\n","                    help=\"target model to use for prediction filepath\")\n","\n","# Get an HIDDEN_UNITS\n","parser.add_argument(\"--hidden_units\",\n","                    default=10,\n","                    type=int,\n","                    help=\"number of neurons in hidden layers\")\n","\n","# Get the arguments from the parser\n","args = parser.parse_args()\n","IMAGE_URL = args.image_url\n","MODEL_PATH = args.model_path\n","HIDDEN_UNITS = args.hidden_units\n","\n","\n","# Setup custom image path\n","data_path = Path(\"data/\")\n","custom_image_path = data_path / \"predict_image.jpeg\"\n","\n","# Download the image if it doesn't already exist\n","if not custom_image_path.is_file():\n","  with open(custom_image_path, \"wb\") as f:\n","    request = requests.get(IMAGE_URL)\n","    print(f\"Downloading {custom_image_path}...\")\n","    f.write(request.content)\n","else:\n","  print(f\"{custom_image_path} already exists, overwriting content...\")\n","  with open(custom_image_path, \"wb\") as f:\n","    request = requests.get(IMAGE_URL)\n","    print(f\"Downloading {custom_image_path}...\")\n","    f.write(request.content)\n","print(\"Done\")\n","\n","\n","class_names = ['cheesecake', 'gnocchi', 'guacamole', 'hamburger', 'paella']\n","\n","# Reload the model\n","try:\n","  model = model_builder.TinyVGG(input_shape=3,\n","                                hidden_units=HIDDEN_UNITS,\n","                                output_shape=len(class_names)).to(device)\n","  model.load_state_dict(torch.load(MODEL_PATH))\n","except:\n","  print(\"Hidden units parameter not matching the loaded model\")\n","\n","\n","# Load the image\n","custom_image = torchvision.io.read_image(str(custom_image_path))\n","\n","# Create transform pipeline to resize image\n","custom_image_transform = transforms.Compose([transforms.Resize(size=(224, 224), antialias=True)])\n","\n","# Transform target image\n","custom_image_transformed = custom_image_transform(custom_image)\n","\n","\n","def pred_and_plot_image(model: torch.nn.Module,\n","                        image_path: str,\n","                        class_names: list[str] = None,\n","                        transform=None,\n","                        device=device):\n","  \"\"\"Makes a prediction on a target image with a trained model and plots the image and prediction.\"\"\"\n","  # Load in the image\n","  target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n","\n","  # Divide the image pixel values by 255 to get them between [0, 1]\n","  target_image = target_image / 255.\n","\n","  # Transform if necessary\n","  if transform:\n","    target_image = transform(target_image)\n","\n","  # Make sure the model is on the target device\n","  model.to(device)\n","\n","  # Turn on eval/inference mode and make a prediction\n","  model.eval()\n","  with torch.inference_mode():\n","    # Add an extra dimension to the image (this is the batch dimension)\n","    target_image = target_image.unsqueeze(0)\n","\n","    # Make a prediciton on the image with an extra dimension\n","    target_image_pred = model(target_image.to(device))\n","\n","  # Convert logits -> prediction probabilities\n","  target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n","\n","  # Convert prediction probabilities -> prediction labels\n","  target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n","\n","  # Plot the image alongside the prediction and prediction probability\n","  print(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max().cpu():.3f}\")\n","\n","# Pred on our custom image\n","pred_and_plot_image(model=model,\n","                    image_path=custom_image_path,\n","                    class_names=class_names,\n","                    transform=custom_image_transform,\n","                    device=device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IUtrRaLlwzm","executionInfo":{"status":"ok","timestamp":1700770896189,"user_tz":-60,"elapsed":248,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"30010bb2-1d73-41ef-e69b-c665e7da7a7f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing modular_approach/predict.py\n"]}]},{"cell_type":"markdown","source":["# Use console and created scripts to download the data"],"metadata":{"id":"O7kSgiRQkeoY"}},{"cell_type":"code","source":["!python modular_approach/data_download.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKpZ9jwiJ36m","executionInfo":{"status":"ok","timestamp":1700770916884,"user_tz":-60,"elapsed":2410,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"210dd207-54ac-4250-fc1c-18e6552a8667"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["data/food101_extract directory created\n","Downloading food101_extract data...\n","Unzipping food101_extract data...\n","Done\n"]}]},{"cell_type":"markdown","source":["# Use console and created scripts to train and evaluate the model\n","\n","Optional - set up custom hyperparameters. Example:\n","\n","`!python train.py --num_epochs 5 --batch_size 32 --hidden_units 20 --learning_rate 0.005`"],"metadata":{"id":"yr9FBmwfkequ"}},{"cell_type":"code","source":["!python modular_approach/main.py --num_epochs 10 --batch_size 16 --hidden_units 20 --learning_rate 0.001"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoaEe1YAketF","executionInfo":{"status":"ok","timestamp":1700771192024,"user_tz":-60,"elapsed":56578,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"f3b866c5-90b8-4f81-9d2c-1f956f348789"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Training a model for 10 epochs with batch size 16 using 20 hidden units and a learning rate of 0.001\n","[INFO] Training data file: data/food101_extract/train\n","[INFO] Testing data file: data/food101_extract/test\n","  0% 0/10 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.6199 | train_acc: 0.2037 | test_loss: 1.5917 | test_acc: 0.1923\n"," 10% 1/10 [00:05<00:51,  5.73s/it]Epoch: 2 | train_loss: 1.5901 | train_acc: 0.2300 | test_loss: 1.5584 | test_acc: 0.2596\n"," 20% 2/10 [00:11<00:47,  5.97s/it]Epoch: 3 | train_loss: 1.5765 | train_acc: 0.2637 | test_loss: 1.5571 | test_acc: 0.2692\n"," 30% 3/10 [00:16<00:36,  5.22s/it]Epoch: 4 | train_loss: 1.5563 | train_acc: 0.2913 | test_loss: 1.5445 | test_acc: 0.2788\n"," 40% 4/10 [00:20<00:28,  4.82s/it]Epoch: 5 | train_loss: 1.5531 | train_acc: 0.2963 | test_loss: 1.5409 | test_acc: 0.3413\n"," 50% 5/10 [00:26<00:26,  5.36s/it]Epoch: 6 | train_loss: 1.5159 | train_acc: 0.3588 | test_loss: 1.5578 | test_acc: 0.2933\n"," 60% 6/10 [00:31<00:20,  5.01s/it]Epoch: 7 | train_loss: 1.4562 | train_acc: 0.4188 | test_loss: 1.5636 | test_acc: 0.3510\n"," 70% 7/10 [00:35<00:14,  4.78s/it]Epoch: 8 | train_loss: 1.3471 | train_acc: 0.4725 | test_loss: 1.5863 | test_acc: 0.3221\n"," 80% 8/10 [00:41<00:10,  5.24s/it]Epoch: 9 | train_loss: 1.2361 | train_acc: 0.5312 | test_loss: 1.7536 | test_acc: 0.3606\n"," 90% 9/10 [00:45<00:04,  4.93s/it]Epoch: 10 | train_loss: 1.1152 | train_acc: 0.5850 | test_loss: 1.8211 | test_acc: 0.3798\n","100% 10/10 [00:50<00:00,  5.00s/it]\n","[INFO] Total training time: 50.018 seconds\n","[INFO] Saving model to: models/05_modular_approach_tinyvgg_model.pth\n","model saved\n"]}]},{"cell_type":"markdown","source":["# Use console to predict on image from a target URL\n","\n","**Warning**: hidden units parameter (default 10) must match with a trained model\n","\n","Example:\n","\n","`!python modular_approach/predict.py --hidden_units 20 --image_url \"https://images/image.jpg\"`"],"metadata":{"id":"yxWbRQU9keve"}},{"cell_type":"code","source":["!python modular_approach/predict.py --hidden_units 20 --image_url \"https://cdn.aniagotuje.com/pictures/articles/2020/05/3989474-v-1500x1500.jpg\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMKDb7ibzK-n","executionInfo":{"status":"ok","timestamp":1700771319789,"user_tz":-60,"elapsed":9294,"user":{"displayName":"Sławomir Wojtas","userId":"02540581615070590880"}},"outputId":"8361922b-9efa-4d7c-b6e0-769306b8df48"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["data/predict_image.jpeg already exists, overwriting content...\n","Downloading data/predict_image.jpeg...\n","Done\n","Pred: hamburger | Prob: 0.271\n"]}]}]}